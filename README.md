
# ğŸ“§ Email Classification System: A Two-Part Exploration
### *DATA 643 - Recommender Systems Course Project*

[![R](https://img.shields.io/badge/R-276DC3?style=flat&logo=r&logoColor=white)](https://www.r-project.org/)
[![Shiny](https://img.shields.io/badge/Shiny-Dashboard-blue)](https://shiny.rstudio.com/)
[![ML](https://img.shields.io/badge/ML-Naive%20Bayes-green)](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
[![NLP](https://img.shields.io/badge/NLP-TF--IDF-orange)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

> **A comprehensive text classification project demonstrating both ambitious problem formulation (personal email categorization) and standard benchmark evaluation (spam detection). This work showcases the value of comparative analysis and the importance of problem selection in machine learning success.**

---

## ğŸ“‹ Assignment Overview

For this project, students were given two options:

### Option 1: Standard Assignment
Build a spam/ham classifier using established public datasets

### Option 2: Ambitious Alternative (Selected)
Use a different labeled dataset of personal choice

**My Approach:** I completed **both options** to gain deeper insights into text classification challenges and to compare performance across problem formulations.

---

## ğŸ¯ Project Structure

### Part 1: Personal Inbox Classification âœ… **COMPLETED**
**Dataset:** My personal Gmail archive (3,200 emails)  
**Categories:** Inbox, Promotions, Social, Updates (4-class problem)  
**Goal:** Understand whether machine learning could automate my email organization

### Part 2: Spam/Ham Classification ğŸ”„ **IN PROGRESS**
**Dataset:** Public spam corpus (10,000+ emails)  
**Categories:** Spam, Ham (2-class problem)  
**Goal:** Compare performance against established benchmarks and test hypothesis about problem complexity

---

## ğŸ“Š Part 1 Results: Personal Inbox Classification

### Motivation

I chose to tackle my personal email organization as the "ambitious" option because:
1. **Real-world relevance** - I actually wanted this functionality
2. **Personalized learning** - My email patterns reflect my actual usage
3. **Challenge** - Multiple similar categories would test model discrimination
4. **Practical application** - Could deploy for daily use via Shiny app

### Methodology

**Data Collection**
- Exported 11,884 emails from my Gmail account
- Categories: Inbox (7,151), Promotions (2,267), Social (845), Updates (3,621)
- Balanced to 800 emails per category to prevent class imbalance bias

**Feature Engineering**
- **Text Preprocessing:** Removed HTML tags, URLs, email addresses, special characters
- **Vectorization:** TF-IDF with top 500 features
- **Document Frequency:** Words appearing in 5+ documents
- **Feature Space:** 500 terms Ã— 2,560 training samples

**Model Training**
- **Algorithm:** Naive Bayes classifier (suited for text classification)
- **Split:** 80% training (2,560 emails), 20% testing (640 emails)
- **Sampling:** Stratified to maintain class proportions

**Evaluation Metrics**
- Overall accuracy
- Per-class precision, recall, F1-score
- Confusion matrix analysis
- Error pattern investigation

### Results

#### Overall Performance
- **Overall Accuracy:** 55.0%
- **Training Time:** ~45 seconds
- **Prediction Time:** <1 second per email

#### Per-Category Performance

| Category | Precision | Recall | F1-Score | Accuracy | Observations |
|----------|-----------|--------|----------|----------|--------------|
| **Social** | 0.87 | 0.87 | 0.87 | **87%** | âœ… Excellent performance |
| **Promotions** | 0.50 | 0.41 | 0.45 | **41%** | âš ï¸ Moderate confusion |
| **Updates** | 0.35 | 0.31 | 0.33 | **31%** | âŒ High confusion |
| **Inbox** | 0.33 | 0.24 | 0.28 | **24%** | âŒ Very high confusion |

#### Confusion Matrix

```
                    Predicted
                Inbox  Promotions  Social  Updates
Actual
Inbox             38      28         0       50      â† 50 confused with Updates
Promotions        29      66        19       17      
Social            50      38       139       43      â† Clear winner
Updates           43      28         2       50      â† 43 confused with Inbox
```

**Visual Representation:**

![Confusion Matrix](confusion_matrix_naive_bayes.png)

### Key Findings & Analysis

#### Finding 1: Category Distinctiveness is Critical

**Social media emails achieved 87% accuracy** because they contain highly distinctive vocabulary:
- Platform-specific terms: "Facebook", "LinkedIn", "Instagram", "Twitter"
- Social actions: "commented", "liked", "tagged", "shared", "mentioned"
- Unique patterns: "@username", "wants to connect", "accepted your request"

These terms rarely appear in other email categories, creating clear decision boundaries.

#### Finding 2: Vocabulary Overlap Causes Confusion

**Inbox, Promotions, and Updates struggled (24-41% accuracy)** due to shared transactional language:
- Order-related: "order", "shipping", "delivery", "tracking", "confirmation"
- Commercial: "save", "discount", "offer", "sale", "deal", "price"
- Action verbs: "click here", "view now", "manage", "update"
- Generic: "account", "email", "information", "service"

**Example of Confusion:**
- **Promotional email:** "Save 50% on your next order! Free shipping over $50."
- **Update email:** "Your order has shipped. Track your delivery here."
- **Personal email:** "Thanks for your order! Here's your receipt."

All three contain "order", "shipping", and calls-to-action, making them linguistically similar.

#### Finding 3: Small Dataset Limitations

With only 800 training samples per category (2,560 total):
- **Feature/sample ratio:** 500 features Ã· 2,560 samples = 0.20
- Risk of overfitting to personal email patterns
- May not generalize to other users' email habits

### Deployment: Interactive Shiny Application

Created a production-ready web application featuring:
- **Real-time classification** of user-input emails
- **File upload support** for .txt files
- **Color-coded predictions** with category descriptions
- **Confidence visualization** for prediction strength

**Technical Implementation:**
```r
predict_email <- function(subject, body_text) {
  full_text <- paste(subject, body_text, sep = " ") %>%
    clean_text() %>%
    create_tfidf_features()
  
  prediction <- predict(nb_model, full_text)
  return(prediction)
}
```

### Lessons Learned from Part 1

**Successes:**
âœ… Completed end-to-end ML pipeline from data collection to deployment  
âœ… Created functional interactive application  
âœ… Identified category-specific performance patterns  
âœ… Gained insights into text similarity challenges  

**Challenges:**
âŒ Overall 55% accuracy insufficient for reliable production use  
âŒ Three categories too similar for effective discrimination  
âŒ Personal email patterns may not generalize well  
âŒ Limited training data per category  

**Critical Insight:**
> *The choice of classification problem matters as much as the model itself. Some categorization schemes are inherently more difficult than others due to vocabulary overlap, regardless of model sophistication.*

---

## ğŸ”¬ Part 2: Spam/Ham Classification (Standard Assignment)

### Hypothesis

Based on findings from Part 1, I hypothesize that spam/ham classification will achieve **significantly higher accuracy (85-95%)** compared to the multi-class inbox classification (55%) for the following reasons:

#### Hypothesis 1: Simpler Decision Boundary
**Prediction:** Binary classification (2 classes) is fundamentally easier than multi-class (4 classes)
- Fewer decision boundaries to learn
- Reduced probability of confusion
- More focused feature learning

#### Hypothesis 2: Greater Category Distinctiveness
**Prediction:** Spam and ham have more distinctive linguistic patterns than inbox/promotions/updates

**Expected Spam Characteristics:**
- Urgency language: "ACT NOW", "LIMITED TIME", "EXPIRES TODAY"
- Poor grammar and spelling errors
- Excessive punctuation: "!!!", "???", "$$$$"
- Suspicious sender patterns (generic email addresses)
- Pharmaceutical/financial terms in unexpected contexts
- ALL CAPS usage

**Expected Ham Characteristics:**
- Normal conversational tone
- Proper grammar and spelling
- Personalized content
- Legitimate sender information
- Context-appropriate vocabulary

**Contrast with Part 1:** Inbox, promotions, and updates all share professional, transactional language with proper grammar, making them harder to distinguish.

#### Hypothesis 3: Larger Training Dataset
**Prediction:** More training data (10,000+ samples vs 3,200) will improve generalization

Public spam datasets offer:
- More diverse spam examples
- Better coverage of spam tactics
- Reduced overfitting risk
- More robust feature learning

#### Hypothesis 4: Established Problem with Known Benchmarks
**Prediction:** Can validate implementation quality against published results

Standard spam detection typically achieves:
- Naive Bayes: 85-95% accuracy
- SVM: 90-98% accuracy
- Deep Learning: 95-99% accuracy

Matching these benchmarks will validate my implementation.

### Planned Methodology

**Dataset Selection**
- Source: SpamAssassin Public Corpus or Enron Email Dataset
- Size: 10,000+ emails (5,000 spam, 5,000 ham)
- Balance: Equal representation to prevent bias

**Consistency with Part 1**
To ensure fair comparison, I will maintain the same:
- Text preprocessing pipeline
- TF-IDF parameters (500 features, 5+ document frequency)
- Model architecture (Naive Bayes)
- Evaluation metrics (accuracy, precision, recall, F1, confusion matrix)
- Train/test split (80/20 with stratified sampling)

**Comparative Analysis**
Will directly compare:
- Part 1 (multi-class): 55% accuracy
- Part 2 (binary): [Expected 85-95%]
- Performance gain quantification
- Feature importance differences
- Error pattern analysis

### Expected Outcomes

**Quantitative Predictions:**
- Overall accuracy: 85-95% (vs 55% in Part 1)
- Precision (spam): >90%
- Recall (spam): >85%
- F1-Score: >87%

**Qualitative Insights:**
- Which features best distinguish spam from ham
- Common false positives (ham classified as spam)
- Common false negatives (spam classified as ham)
- Comparison of error patterns between Part 1 and Part 2

**Research Value:**
- Demonstrates impact of problem formulation on ML success
- Shows importance of category distinctiveness
- Validates hypothesis-driven experimental approach

### Timeline

- [x] Part 1: Personal inbox classification (Completed)
- [x] Part 1: Shiny application deployment (Completed)
- [x] Part 1: Results analysis and documentation (Completed)
- [ ] Part 2: Dataset acquisition and preparation (In Progress)
- [ ] Part 2: Model training and evaluation
- [ ] Part 2: Comparative analysis with Part 1
- [ ] Part 2: Final report and conclusions

**Expected Completion:** [Your Date]

---

## ğŸ› ï¸ Technical Implementation

### Shared Architecture (Both Parts)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA INGESTION                            â”‚
â”‚  Part 1: Personal Gmail Export â†’ CSV Files                 â”‚
â”‚  Part 2: Public Spam Corpus â†’ CSV Files                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TEXT PREPROCESSING                           â”‚
â”‚  â€¢ Remove HTML tags, URLs, email addresses                 â”‚
â”‚  â€¢ Convert to lowercase                                     â”‚
â”‚  â€¢ Strip special characters                                 â”‚
â”‚  â€¢ Tokenization                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             FEATURE ENGINEERING (TF-IDF)                    â”‚
â”‚  â€¢ Top 500 features                                         â”‚
â”‚  â€¢ Document frequency: 5-Inf                                â”‚
â”‚  â€¢ Sparse matrix representation                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             MODEL TRAINING (NAIVE BAYES)                    â”‚
â”‚  â€¢ 80/20 Train/Test Split                                   â”‚
â”‚  â€¢ Stratified Sampling                                      â”‚
â”‚  â€¢ Class balancing                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        EVALUATION & ANALYSIS                                â”‚
â”‚  â€¢ Confusion Matrix                                         â”‚
â”‚  â€¢ Precision, Recall, F1-Score                              â”‚
â”‚  â€¢ Error Analysis                                           â”‚
â”‚  â€¢ Comparative Analysis (Part 1 vs Part 2)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Programming Language:** R 4.x

**Core Libraries:**
- `tidyverse` - Data manipulation and visualization
- `tm` - Text mining and preprocessing
- `e1071` - Naive Bayes classifier
- `caret` - Machine learning workflow and evaluation
- `tidytext` - Text analysis and tokenization
- `yardstick` - Model performance metrics

**Deployment:**
- `shiny` - Interactive web application
- `shinyapps.io` - Cloud hosting (optional)

**Documentation:**
- `rmarkdown` - Reproducible analysis notebooks
- `knitr` - Dynamic report generation

---

## ğŸ“ˆ Comparative Analysis Framework

### Metrics for Comparison

| Metric | Part 1: Inbox (4-class) | Part 2: Spam (2-class) | Expected Î” |
|--------|-------------------------|------------------------|------------|
| Overall Accuracy | 55% | [TBD] | +30-40% |
| Best Category | 87% (Social) | [TBD] | Similar |
| Worst Category | 24% (Inbox) | [TBD] | +50%+ |
| F1-Score (Macro Avg) | 0.48 | [TBD] | +0.35 |
| Training Samples | 2,560 | ~8,000 | 3.1x more |
| Features | 500 | 500 | Same |

### Analysis Questions

1. **Does binary classification significantly outperform multi-class?**
   - Compare overall accuracy between parts
   - Analyze confusion matrix patterns

2. **Are spam/ham more linguistically distinct than inbox categories?**
   - Compare per-class accuracy
   - Examine top discriminative features

3. **Does larger dataset improve performance?**
   - Compare learning curves
   - Analyze overfitting indicators

4. **What are common error patterns in each task?**
   - False positives vs false negatives
   - Systematic misclassification patterns

---

## ğŸ“ Learning Outcomes

### Technical Skills Demonstrated

**Part 1: Personal Inbox Classification**
âœ… Data collection from real-world source (Gmail)  
âœ… Handling imbalanced datasets (7,151 inbox vs 845 social)  
âœ… Multi-class classification implementation  
âœ… Interactive application deployment (Shiny)  
âœ… Production-ready software development  

**Part 2: Spam/Ham Classification**
âœ… Working with public benchmark datasets  
âœ… Binary classification optimization  
âœ… Comparative experimental design  
âœ… Hypothesis formulation and testing  
âœ… Performance against established baselines  

### Research Methodology

âœ… **Problem Formulation:** Defined classification taxonomies for different use cases  
âœ… **Critical Analysis:** Identified why certain categories perform better  
âœ… **Hypothesis Generation:** Developed testable predictions about spam detection  
âœ… **Experimental Design:** Created controlled comparison between problem types  
âœ… **Documentation:** Clear, reproducible reporting of methods and findings  

### Domain Insights

âœ… **Text Classification Fundamentals:** Understanding of TF-IDF, Naive Bayes, evaluation metrics  
âœ… **Problem Difficulty:** Recognition that some classification tasks are inherently harder  
âœ… **Feature Importance:** Appreciation for distinctive vs overlapping vocabularies  
âœ… **Practical Deployment:** Experience building usable ML applications  
âœ… **Iterative Improvement:** Using results to inform next experiments  

---

## ğŸš€ Reproducibility Guide

### Running Part 1: Personal Inbox Classification

```r
# 1. Install required packages
install.packages(c("tidyverse", "tm", "caret", "e1071", "shiny", "tidytext"))

# 2. Prepare your email data
# Export Gmail emails as CSV files: inbox.csv, promotions.csv, social.csv, updates.csv
# Place in project directory

# 3. Run analysis notebook
rmarkdown::render("gmail_classification_simple.Rmd")

# 4. Save trained model
saveRDS(nb_model, "nb_model.rds")
saveRDS(top_terms, "top_terms.rds")
saveRDS(train_features, "train_features.rds")

# 5. Launch interactive application
shiny::runApp("email_classifier_app.R")
```

### Running Part 2: Spam/Ham Classification

```r
# 1. Download public spam dataset
# Options: SpamAssassin, Enron Email Dataset

# 2. Run spam classification notebook
# [Coming soon after data preparation]
rmarkdown::render("spam_classification.Rmd")

# 3. Run comparative analysis
rmarkdown::render("comparative_analysis.Rmd")
```

---

## ğŸ“ Repository Structure

```
email-classification-project/
â”‚
â”œâ”€â”€ Part 1: Personal Inbox Classification [COMPLETED]
â”‚   â”œâ”€â”€ ğŸ“Š gmail_classification_simple.Rmd    # Main analysis
â”‚   â”œâ”€â”€ ğŸ¨ email_classifier_app.R             # Shiny app
â”‚   â”œâ”€â”€ ğŸ“ˆ confusion_matrix_naive_bayes.png   # Results
â”‚   â”œâ”€â”€ ğŸ“ˆ email_distribution.png             # Visualizations
â”‚   â”œâ”€â”€ ğŸ“ˆ email_length_distribution.png
â”‚   â”œâ”€â”€ ğŸ“ˆ top_words_by_category.png
â”‚   â”œâ”€â”€ ğŸ¤– nb_model.rds                       # Trained model
â”‚   â”œâ”€â”€ ğŸ¤– top_terms.rds                      # Vocabulary
â”‚   â””â”€â”€ ğŸ¤– train_features.rds                 # Feature structure
â”‚
â”œâ”€â”€ Part 2: Spam/Ham Classification [IN PROGRESS]
â”‚   â”œâ”€â”€ ğŸ“Š spam_classification.Rmd            # Analysis notebook
â”‚   â”œâ”€â”€ ğŸ“Š comparative_analysis.Rmd           # Part 1 vs Part 2
â”‚   â””â”€â”€ ğŸ“Š results_comparison.Rmd             # Final comparison
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ inbox.csv, promotions.csv, social.csv, updates.csv  # Part 1
â”‚   â””â”€â”€ spam.csv, ham.csv                     # Part 2 [TBD]
â”‚
â””â”€â”€ ğŸ“– README.md                               # This file
```

---

## ğŸ“Š Key Visualizations

### Part 1 Results

**Email Distribution by Category**
![Email Distribution](email_distribution.png)

**Confusion Matrix**
![Confusion Matrix](confusion_matrix_naive_bayes.png)

**Top Discriminative Words**
![Top Words](top_words_by_category.png)

### Part 2 Results
*[To be added upon completion]*

---

## ğŸ¯ Project Significance

### Why This Dual Approach Matters

**Academic Value:**
- Demonstrates thorough understanding of text classification
- Shows ability to work with both custom and standard datasets
- Provides comparative analysis of problem difficulty
- Validates hypotheses through controlled experimentation

**Practical Value:**
- Part 1 solves real personal email organization problem
- Part 2 demonstrates capability on industry-standard task
- Combined work shows versatility in problem-solving
- Interactive application demonstrates deployment skills

**Research Contribution:**
- Quantifies impact of problem formulation on classifier performance
- Demonstrates importance of category distinctiveness
- Provides insights into vocabulary overlap challenges
- Offers practical guidance for future email classification projects

---

## ğŸ“š References & Related Work

### Foundational Papers
- Rennie, J. et al. (2003). "Tackling the Poor Assumptions of Naive Bayes Text Classifiers"
- Androutsopoulos, J. et al. (2000). "Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a Memory-Based Approach"
- Sahami, M. et al. (1998). "A Bayesian Approach to Filtering Junk E-Mail"

### Datasets
- [SpamAssassin Public Corpus](https://spamassassin.apache.org/old/publiccorpus/)
- [Enron Email Dataset](https://www.cs.cmu.edu/~enron/)
- Personal Gmail Archive (Part 1)

### Tools & Libraries
- [tm: Text Mining Package](https://cran.r-project.org/web/packages/tm/)
- [tidytext: Text Mining using dplyr, ggplot2](https://www.tidytextmining.com/)
- [caret: Classification and Regression Training](https://topepo.github.io/caret/)

---

## ğŸ”„ Next Steps

### Immediate (Part 2 Completion)
- [ ] Acquire and prepare spam/ham dataset
- [ ] Train Naive Bayes model with consistent parameters
- [ ] Evaluate performance and compare with Part 1
- [ ] Document findings and validate/refute hypotheses
- [ ] Create visualizations comparing both approaches

### Future Enhancements
- [ ] Implement additional classifiers (SVM, Random Forest, LSTM)
- [ ] Feature engineering: Add metadata (sender domain, time patterns)
- [ ] Ensemble methods: Combine multiple models
- [ ] Deploy spam classifier as web service
- [ ] Extend Part 1 with feature engineering for confused categories

---

## ğŸ‘¤ Author Information

**Student:** Candace  
**Course:** DATA 643 - Recommender Systems  
**Institution:** CUNY School of Professional Studies  
**Program:** Master's in Data Science  
**Semester:** [Your Semester]

### Project Status
- âœ… Part 1: Personal Inbox Classification - **COMPLETED**
- ğŸ”„ Part 2: Spam/Ham Classification - **IN PROGRESS**
- â³ Comparative Analysis - **PENDING PART 2 COMPLETION**

---

## ğŸ“ Acknowledgments

I would like to thank:
- **Course Instructor** for offering the ambitious alternative option
- **Gmail** for data export functionality
- **R Community** for excellent text mining packages
- **SpamAssassin/Enron** for public spam datasets

---

## ğŸ“œ Academic Integrity Statement

This project represents my original work completed for DATA 643. All data collection, analysis, implementation, and documentation were performed independently. External resources and references are properly cited throughout.

---

<div align="center">

### ğŸ“§ From Personal Emails to Spam Detection ğŸ“§

*Exploring text classification through comparative problem analysis*

**Part 1 Complete | Part 2 In Progress | Results Pending**

*Built with R â€¢ Powered by Curiosity â€¢ Validated by Data*

</div>

---

**Last Updated:** November 2024  
**Status:** Part 1 Complete, Part 2 In Progress
